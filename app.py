import json
import os
import time
import typing as t
import csv
import re
import random
import uuid
from collections import Counter
from datetime import datetime

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from flask import Flask, Response, jsonify, make_response, request

# OpenAI (AI writer)
from openai import OpenAI

# ------------------------ Config / Tokens ------------------------

VERIFY_TOKEN = os.getenv("WEBHOOK_VERIFY_TOKEN", "AKUTA_2025_SECURE_TOKEN")
SECRET_KEY = os.getenv("SECRET_KEY", "akuta_secure_key_2025")
TOKENS_FILE = os.getenv("TOKENS_FILE", "/etc/secrets/tokens.json")
DISABLE_SSE = os.getenv("DISABLE_SSE", "1") not in ("0", "false", "False")

OPENAI_API_KEY  = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL    = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

# --- body length config ---
BODY_MIN_WORDS = int(os.getenv("BODY_MIN_WORDS", "160"))
BODY_MAX_WORDS = int(os.getenv("BODY_MAX_WORDS", "260"))

# Anti-dup
ANTI_DUP_ENABLED = os.getenv("ANTI_DUP_ENABLED", "1") not in ("0","false","False")
DUP_J_THRESHOLD  = float(os.getenv("DUP_J", "0.35"))
DUP_L_THRESHOLD  = float(os.getenv("DUP_L", "0.90"))
MAX_TRIES_ENV    = int(os.getenv("MAX_TRIES", "5"))

# File paths
CORPUS_FILE     = os.getenv("CORPUS_FILE", "/tmp/post_corpus.json")
SETTINGS_FILE = os.getenv('SETTINGS_FILE', '/tmp/page_settings.json')
UPLOAD_FOLDER = os.getenv('UPLOAD_FOLDER', '/tmp/uploads')

app = Flask(__name__)
app.secret_key = SECRET_KEY

# T·∫°o th∆∞ m·ª•c upload n·∫øu ch∆∞a t·ªìn t·∫°i
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def _load_settings():
    """T·∫£i c√†i ƒë·∫∑t t·ª´ file JSON ho·∫∑c CSV"""
    try:
        with open(SETTINGS_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    except FileNotFoundError:
        pass
    
    data = {}
    if os.path.exists('settings.csv'):
        try:
            with open('settings.csv', newline='', encoding='utf-8') as f:
                rdr = csv.DictReader(f)
                for row in rdr:
                    pid = (row.get('id') or '').strip()
                    if not pid:
                        continue
                    data[pid] = {
                        "keyword": (row.get('keyword') or row.get('keywords') or '').strip(),
                        "source":  (row.get('source')  or row.get('link')     or '').strip(),
                    }
            _save_settings(data)
            return data
        except Exception:
            pass
    return {}

def _ensure_dir_for(path: str):
    """ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i"""
    d = os.path.dirname(path)
    if d and not os.path.exists(d):
        os.makedirs(d, exist_ok=True)

def _save_settings(data: dict):
    """L∆∞u c√†i ƒë·∫∑t v√†o file"""
    _ensure_dir_for(SETTINGS_FILE)
    try:
        with open(SETTINGS_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        # Fallback: l∆∞u v√†o th∆∞ m·ª•c hi·ªán t·∫°i
        with open('./page_settings_fallback.json', 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

# Facebook API Configuration
FB_CONNECT_TIMEOUT = float(os.getenv("FB_CONNECT_TIMEOUT", "5"))
FB_READ_TIMEOUT    = float(os.getenv("FB_READ_TIMEOUT", "45"))
FB_RETRIES         = int(os.getenv("FB_RETRIES", "3"))
FB_BACKOFF         = float(os.getenv("FB_BACKOFF", "0.5"))
FB_POOL            = int(os.getenv("FB_POOL", "50"))

# Reuse connections + retries
session = requests.Session()
retry = Retry(
    total=FB_RETRIES,
    connect=FB_RETRIES,
    read=FB_RETRIES,
    backoff_factor=FB_BACKOFF,
    status_forcelist=[429,500,502,503,504],
    allowed_methods=frozenset(["GET","POST"])
)
adapter = HTTPAdapter(pool_connections=FB_POOL, pool_maxsize=FB_POOL, max_retries=retry)
session.mount("https://", adapter)
session.mount("http://", adapter)

def _load_tokens() -> dict:
    """T·∫£i tokens t·ª´ bi·∫øn m√¥i tr∆∞·ªùng ho·∫∑c file"""
    env_json = os.getenv("PAGE_TOKENS")
    if env_json:
        try:
            return json.loads(env_json)
        except Exception:
            pass
    
    try:
        with open(TOKENS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
        if isinstance(data, dict) and "pages" in data and isinstance(data["pages"], dict):
            return data["pages"]
        if isinstance(data, dict):
            return data
    except Exception:
        pass
    
    # Fallback: ki·ªÉm tra file trong th∆∞ m·ª•c hi·ªán t·∫°i
    try:
        with open("./tokens_fallback.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            return data.get("pages", data)
    except Exception:
        pass
    
    return {}

PAGE_TOKENS = _load_tokens()

def get_page_token(page_id: str) -> str:
    """L·∫•y token cho page_id"""
    token = PAGE_TOKENS.get(page_id, "")
    if not token:
        raise RuntimeError(f"Kh√¥ng t√¨m th·∫•y token cho page_id={page_id}")
    return token

# ------------------------ Facebook Graph API Helpers ------------------------

FB_VERSION = "v20.0"
FB_API = f"https://graph.facebook.com/{FB_VERSION}"

def fb_get(path: str, params: dict, timeout: int = 30) -> dict:
    """Th·ª±c hi·ªán GET request ƒë·∫øn Facebook Graph API"""
    url = f"{FB_API}/{path.lstrip('/')}"
    r = session.get(url, params=params, timeout=(FB_CONNECT_TIMEOUT, FB_READ_TIMEOUT))
    try:
        data = r.json()
    except Exception:
        data = {"error": {"message": f"HTTP {r.status_code} (no json)"}}
    if r.status_code >= 400 or "error" in data:
        raise RuntimeError(f"FB GET {url} failed: {data}")
    return data

def fb_post(path: str, data: dict, timeout: int = 30) -> dict:
    """Th·ª±c hi·ªán POST request ƒë·∫øn Facebook Graph API"""
    url = f"{FB_API}/{path.lstrip('/')}"
    r = session.post(url, data=data, timeout=(FB_CONNECT_TIMEOUT, FB_READ_TIMEOUT))
    try:
        js = r.json()
    except Exception:
        js = {"error": {"message": f"HTTP {r.status_code} (no json)"}}
    if r.status_code >= 400 or "error" in js:
        raise RuntimeError(f"FB POST {url} failed: {js}")
    return js

# ------------------------ AI Content Writer (Enhanced Version) ------------------------

class AIContentWriter:
    def __init__(self, openai_client):
        self.client = openai_client
        self.content_framework = {
            'problems': {
                'financial': ['m·∫•t ƒëi·ªÉm', 'kh√≥a t√†i kho·∫£n', 'r√∫t ti·ªÅn th·∫•t b·∫°i', 'giao d·ªãch treo', 
                            'th·∫•t l·∫°c giao d·ªãch', 'kh√¥ng th·ªÉ r√∫t ti·ªÅn', 's·ªë d∆∞ bi·∫øn m·∫•t', 'l·ªói n·∫°p ti·ªÅn'],
                'technical': ['b·ªã ch·∫∑n link', 'kh√¥ng th·ªÉ truy c·∫≠p', 'k·∫øt n·ªëi gi√°n ƒëo·∫°n', 'l·ªói k·∫øt n·ªëi', 
                            'm·∫•t k·∫øt n·ªëi', 'truy c·∫≠p b·ªã t·ª´ ch·ªëi', 'l·ªói h·ªá th·ªëng', 'b·∫£o tr√¨'],
                'security': ['b·∫£o m·∫≠t', 'x√°c th·ª±c', 'b·∫£o v·ªá t√†i kho·∫£n', 'ƒëƒÉng nh·∫≠p th·∫•t b·∫°i', 
                           't√†i kho·∫£n b·ªã ƒë√°nh c·∫Øp', 'th√¥ng tin c√° nh√¢n', 'x√°c minh danh t√≠nh']
            },
            'solutions': {
                'speed': ['nhanh ch√≥ng', 't·ª©c th√¨', 'trong t√≠ch t·∫Øc', 'ngay l·∫≠p t·ª©c', 'kh·∫©n tr∆∞∆°ng', 'nhanh g·ªçn'],
                'quality': ['chuy√™n nghi·ªáp', 'ch√≠nh x√°c', 't·∫≠n t√¢m', 'chu ƒë√°o', 't·∫≠n t√¨nh', 'c·∫©n th·∫≠n', 't·ªâ m·ªâ'],
                'security': ['b·∫£o m·∫≠t', 'an to√†n', 'ri√™ng t∆∞', 'b√≠ m·∫≠t', 'b·∫£o v·ªá', 'm√£ h√≥a', 'x√°c th·ª±c']
            },
            'tones': {
                'urgent': "üî¥ S·ª± c·ªë c·∫ßn gi·∫£i quy·∫øt NGAY?",
                'friendly': "üí¨ B·∫°n ƒëang g·∫∑p ch√∫t r·∫Øc r·ªëi?",
                'professional': "‚ö° H·ªó tr·ª£ chuy√™n nghi·ªáp cho m·ªçi v·∫•n ƒë·ªÅ",
                'reassuring': "üõ°Ô∏è ƒê·ª´ng lo - Ch√∫ng t√¥i ·ªü ƒë√¢y ƒë·ªÉ gi√∫p b·∫°n",
                'empowering': "üöÄ Kh·∫Øc ph·ª•c m·ªçi tr·ªü ng·∫°i c√πng chuy√™n gia"
            }
        }
        
        self.benefit_variations = [
            {"icon": "üöÄ", "keywords": ["t·ªëc ƒë·ªô", "nhanh", "kh·∫©n", "x·ª≠ l√Ω t·ª©c th√¨"]},
            {"icon": "üõ°Ô∏è", "keywords": ["b·∫£o m·∫≠t", "an to√†n", "ri√™ng t∆∞", "b·∫£o v·ªá"]},
            {"icon": "üìû", "keywords": ["24/7", "h·ªó tr·ª£", "t∆∞ v·∫•n", "chƒÉm s√≥c"]},
            {"icon": "üîÑ", "keywords": ["theo s√°t", "ƒë·ªìng h√†nh", "xuy√™n su·ªët", "li√™n t·ª•c"]},
            {"icon": "üíØ", "keywords": ["mi·ªÖn ph√≠", "ch·∫•t l∆∞·ª£ng", "uy t√≠n", "ƒë·∫£m b·∫£o"]},
            {"icon": "‚úÖ", "keywords": ["cam k·∫øt", "ho√†n t·∫•t", "tri·ªát ƒë·ªÉ", "ch·∫Øc ch·∫Øn"]},
            {"icon": "üåê", "keywords": ["·ªïn ƒë·ªãnh", "li√™n t·ª•c", "th√¥ng su·ªët", "m∆∞·ª£t m√†"]},
            {"icon": "‚ö°", "keywords": ["x·ª≠ l√Ω", "ph·∫£n h·ªìi", "kh·∫©n c·∫•p", "nhanh ch√≥ng"]},
            {"icon": "üë®‚Äçüíº", "keywords": ["chuy√™n gia", "chuy√™n nghi·ªáp", "kinh nghi·ªám", "tay ngh·ªÅ"]},
            {"icon": "üîê", "keywords": ["m√£ h√≥a", "b·∫£o v·ªá", "an ninh", "x√°c th·ª±c"]},
            {"icon": "üìä", "keywords": ["minh b·∫°ch", "r√µ r√†ng", "chi ti·∫øt", "c√¥ng khai"]},
            {"icon": "üéØ", "keywords": ["ch√≠nh x√°c", "hi·ªáu qu·∫£", "t·ªëi ∆∞u", "ph√π h·ª£p"]}
        ]

    def generate_smart_title(self):
        """T·∫°o ti√™u ƒë·ªÅ th√¥ng minh v·ªõi nhi·ªÅu bi·∫øn th·ªÉ"""
        base_templates = [
            "‚ùñ {year} - {feature1} & {feature2} | K·∫øt n·ªëi {quality}",
            "‚ùñ Tr·∫£i nghi·ªám {adjective} {year} - {benefit}",
            "‚ùñ {platform} {year} - {promise1} v√† {promise2}",
            "‚ùñ Gateway {year}: {focus} v·ªõi {advantage}",
            "‚ùñ {platform} Premium {year}: {value1} + {value2}",
            "‚ùñ N√¢ng c·∫•p {year} - {improvement1} v√† {improvement2}",
            "‚ùñ {platform} {year}: {slogan1} c√πng {slogan2}",
            "‚ùñ K·∫øt n·ªëi {year}: {attribute1} & {attribute2}"
        ]
        
        features = ["B·∫£o m·∫≠t t·ªëi ƒëa", "T·ªëc ƒë·ªô cao", "·ªîn ƒë·ªãnh tuy·ªát ƒë·ªëi", "K·∫øt n·ªëi th√¥ng minh", 
                   "H·ªó tr·ª£ chuy√™n s√¢u", "Hi·ªáu su·∫•t v∆∞·ª£t tr·ªôi", "C√¥ng ngh·ªá m·ªõi"]
        qualities = ["m∆∞·ª£t m√†", "li·ªÅn m·∫°ch", "an to√†n", "nhanh ch√≥ng", "·ªïn ƒë·ªãnh", "b·∫£o m·∫≠t"]
        adjectives = ["v∆∞·ª£t tr·ªôi", "kh√°c bi·ªát", "∆∞u vi·ªát", "ho√†n h·∫£o", "cao c·∫•p", "chuy√™n nghi·ªáp"]
        benefits = ["b·∫£o m·∫≠t ƒë·ªânh cao", "t·ªëc ƒë·ªô v∆∞·ª£t tr·ªôi", "tr·∫£i nghi·ªám m∆∞·ª£t m√†", 
                   "h·ªó tr·ª£ t·ª©c th√¨", "k·∫øt n·ªëi ·ªïn ƒë·ªãnh", "d·ªãch v·ª• ho√†n h·∫£o"]
        
        template = random.choice(base_templates)
        return template.format(
            year="2025",
            feature1=random.choice(features),
            feature2=random.choice(features),
            quality=random.choice(qualities),
            adjective=random.choice(adjectives),
            benefit=random.choice(benefits),
            platform="JB88",
            promise1=random.choice(["K·∫øt n·ªëi b·∫£o m·∫≠t", "ƒê∆∞·ªùng link ch√≠nh ch·ªß", "Truy c·∫≠p an to√†n", "H·ªá th·ªëng ·ªïn ƒë·ªãnh"]),
            promise2=random.choice(["h·ªó tr·ª£ 24/7", "x·ª≠ l√Ω t·ª©c th√¨", "gi·∫£i ph√°p to√†n di·ªán", "d·ªãch v·ª• chuy√™n nghi·ªáp"]),
            focus=random.choice(["B·∫£o m·∫≠t", "T·ªëc ƒë·ªô", "·ªîn ƒë·ªãnh", "Hi·ªáu su·∫•t", "Ch·∫•t l∆∞·ª£ng"]),
            advantage=random.choice(["c√¥ng ngh·ªá m·ªõi", "ƒë·ªôi ng≈© chuy√™n gia", "h·ªá th·ªëng t·ªëi ∆∞u", "gi·∫£i ph√°p th√¥ng minh"]),
            value1=random.choice(["B·∫£o m·∫≠t c·∫•p cao", "T·ªëc ƒë·ªô v∆∞·ª£t tr·ªôi", "K·∫øt n·ªëi ·ªïn ƒë·ªãnh"]),
            value2=random.choice(["H·ªó tr·ª£ chuy√™n s√¢u", "Tr·∫£i nghi·ªám c√° nh√¢n h√≥a", "D·ªãch v·ª• t·∫≠n t√¢m"]),
            improvement1=random.choice(["t·ªëc ƒë·ªô x·ª≠ l√Ω", "b·∫£o m·∫≠t d·ªØ li·ªáu", "tr·∫£i nghi·ªám ng∆∞·ªùi d√πng"]),
            improvement2=random.choice(["ƒë·ªô ·ªïn ƒë·ªãnh", "kh·∫£ nƒÉng ti·∫øp c·∫≠n", "h·ªó tr·ª£ kh√°ch h√†ng"]),
            slogan1=random.choice(["An to√†n tuy·ªát ƒë·ªëi", "B·∫£o m·∫≠t t·ªëi ∆∞u", "K·∫øt n·ªëi li·ªÅn m·∫°ch"]),
            slogan2=random.choice(["h·ªó tr·ª£ chuy√™n nghi·ªáp", "gi·∫£i ph√°p to√†n di·ªán", "d·ªãch v·ª• ƒë·∫≥ng c·∫•p"]),
            attribute1=random.choice(["B·∫£o m·∫≠t", "T·ªëc ƒë·ªô", "·ªîn ƒë·ªãnh"]),
            attribute2=random.choice(["An to√†n", "Hi·ªáu qu·∫£", "Chuy√™n nghi·ªáp"])
        )

    def generate_contextual_description(self):
        """T·∫°o m√¥ t·∫£ ng·ªØ c·∫£nh th√¥ng minh"""
        problem_type = random.choice(list(self.content_framework['problems'].keys()))
        problems = self.content_framework['problems'][problem_type]
        
        solution_type = random.choice(list(self.content_framework['solutions'].keys()))
        solutions = self.content_framework['solutions'][solution_type]
        
        tone = random.choice(list(self.content_framework['tones'].values()))
        
        description_templates = [
            f"{tone} ƒêang g·∫∑p v·∫•n ƒë·ªÅ v·ªÅ **{', '.join(random.sample(problems, 2))}**? ƒê·ªôi ng≈© c·ªßa ch√∫ng t√¥i cam k·∫øt gi·∫£i quy·∫øt {random.choice(solutions)} v·ªõi quy tr√¨nh chuy√™n nghi·ªáp v√† b·∫£o m·∫≠t. Ch√∫ng t√¥i hi·ªÉu r·∫±ng m·ªói ph√∫t gi√¢y ƒë·ªÅu qu√Ω gi√° v√† s·∫Ω n·ªó l·ª±c h·∫øt m√¨nh ƒë·ªÉ kh√¥i ph·ª•c tr·∫£i nghi·ªám c·ªßa b·∫°n trong th·ªùi gian ng·∫Øn nh·∫•t.",
            
            f"Kh√¥ng th·ªÉ **{random.choice(problems)}**? ƒê·ª´ng ƒë·ªÉ ƒëi·ªÅu n√†y l√†m gi√°n ƒëo·∫°n tr·∫£i nghi·ªám c·ªßa b·∫°n! H·ªá th·ªëng h·ªó tr·ª£ {random.choice(solutions)} c·ªßa ch√∫ng t√¥i lu√¥n s·∫µn s√†ng. V·ªõi ƒë·ªôi ng≈© chuy√™n gia gi√†u kinh nghi·ªám, ch√∫ng t√¥i s·∫Ω ƒë·ªìng h√†nh c√πng b·∫°n t·ª´ b∆∞·ªõc ƒë·∫ßu ti√™n cho ƒë·∫øn khi v·∫•n ƒë·ªÅ ƒë∆∞·ª£c gi·∫£i quy·∫øt ho√†n to√†n.",
            
            f"T·ª´ **{problems[0]}** ƒë·∫øn **{problems[-1]}** - m·ªçi th√°ch th·ª©c ƒë·ªÅu c√≥ gi·∫£i ph√°p. Ph∆∞∆°ng ch√¢m c·ªßa ch√∫ng t√¥i: x·ª≠ l√Ω {random.choice(solutions)} - b·∫£o m·∫≠t tuy·ªát ƒë·ªëi. Ch√∫ng t√¥i kh√¥ng ch·ªâ kh·∫Øc ph·ª•c s·ª± c·ªë m√† c√≤n ƒë·∫£m b·∫£o tr·∫£i nghi·ªám c·ªßa b·∫°n ƒë∆∞·ª£c c·∫£i thi·ªán t·ªët h∆°n sau m·ªói l·∫ßn h·ªó tr·ª£.",
            
            f"Tr·∫£i nghi·ªám d·ªãch v·ª• {random.choice(solutions)} ƒë·∫≥ng c·∫•p. D√π b·∫°n ƒëang ƒë·ªëi m·∫∑t v·ªõi **{random.choice(problems)}** hay b·∫•t k·ª≥ v·∫•n ƒë·ªÅ n√†o kh√°c, ch√∫ng t√¥i ƒë·ªÅu c√≥ gi·∫£i ph√°p ph√π h·ª£p. M·ªói tr∆∞·ªùng h·ª£p ƒë·ªÅu ƒë∆∞·ª£c ph√¢n t√≠ch k·ªπ l∆∞·ª°ng v√† x·ª≠ l√Ω v·ªõi s·ª± t·∫≠n t√¢m cao nh·∫•t."
        ]
        
        return random.choice(description_templates)

    def generate_dynamic_benefits(self):
        """T·∫°o danh s√°ch l·ª£i √≠ch ƒë·ªông"""
        num_benefits = random.randint(6, 8)
        selected_benefits = random.sample(self.benefit_variations, num_benefits)
        
        benefit_texts = []
        for benefit in selected_benefits:
            base_text = benefit['keywords'][0]
            if len(benefit['keywords']) > 1:
                modifier = random.choice(benefit['keywords'][1:])
                templates = [
                    f"{base_text} {modifier}",
                    f"{modifier} trong {base_text}",
                    f"ƒë·∫£m b·∫£o {base_text} {modifier}",
                    f"{modifier} - {base_text} tuy·ªát ƒë·ªëi",
                    f"gi·∫£i ph√°p {base_text} {modifier}",
                    f"cam k·∫øt {base_text} {modifier}",
                    f"{base_text} {modifier} h√†ng ƒë·∫ßu"
                ]
                text = random.choice(templates)
            else:
                text = base_text
                
            benefit_texts.append(f"{benefit['icon']} {text.title()}")
        
        return benefit_texts

    def generate_smart_cta(self, context):
        """T·∫°o CTA th√¥ng minh d·ª±a tr√™n ng·ªØ c·∫£nh"""
        urgent_keywords = ['kh·∫©n', 'ngay l·∫≠p t·ª©c', 't·ª©c th√¨', 'g·∫•p', 'kh·∫©n c·∫•p']
        is_urgent = any(keyword in context.lower() for keyword in urgent_keywords)
        
        if is_urgent:
            ctas = [
                "‚è∞ **Th·ªùi gian l√† v√†ng!** Li√™n h·ªá ngay ƒë·ªÉ ƒë∆∞·ª£c ∆∞u ti√™n x·ª≠ l√Ω v√† kh√¥i ph·ª•c tr·∫°ng th√°i nhanh ch√≥ng.",
                "üö® **T√¨nh hu·ªëng kh·∫©n c·∫•p?** Ph·∫£n h·ªìi ngay l·∫≠p t·ª©c khi b·∫°n li√™n h·ªá - ƒë·ªôi ng≈© chuy√™n gia s·∫µn s√†ng h·ªó tr·ª£.",
                "‚ö° **C·∫ßn gi·∫£i quy·∫øt g·∫•p?** Ch√∫ng t√¥i ∆∞u ti√™n c√°c tr∆∞·ªùng h·ª£p nh∆∞ b·∫°n v√† cam k·∫øt x·ª≠ l√Ω trong th·ªùi gian ng·∫Øn nh·∫•t.",
                "üî¥ **Kh√¥ng th·ªÉ ch·ªù ƒë·ª£i?** H·ªó tr·ª£ t·ª©c th√¨ - g·ªçi ngay ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n v√† h∆∞·ªõng d·∫´n chi ti·∫øt!"
            ]
        else:
            ctas = [
                "üí¨ **S·∫µn s√†ng h·ªó tr·ª£!** ƒê·ªÉ l·∫°i th√¥ng tin ƒë·ªÉ ƒë∆∞·ª£c t∆∞ v·∫•n chi ti·∫øt v√† gi·∫£i ph√°p ph√π h·ª£p nh·∫•t.",
                "ü§ù **K·∫øt n·ªëi ngay h√¥m nay** ƒë·ªÉ tr·∫£i nghi·ªám d·ªãch v·ª• ƒë·∫≥ng c·∫•p v√† chuy√™n nghi·ªáp t·ª´ ƒë·ªôi ng≈© gi√†u kinh nghi·ªám.",
                "üìû **ƒê·ª´ng ng·∫ßn ng·∫°i** - ƒê·ªôi ng≈© chuy√™n gia lu√¥n s·∫µn s√†ng l·∫Øng nghe v√† ƒë∆∞a ra gi·∫£i ph√°p t·ªëi ∆∞u cho b·∫°n.",
                "üåü **B·∫Øt ƒë·∫ßu ngay** - Gi·∫£i ph√°p ho√†n h·∫£o ƒëang ch·ªù b·∫°n kh√°m ph√° v·ªõi s·ª± h·ªó tr·ª£ t·∫≠n t√¢m t·ª´ ch√∫ng t√¥i."
            ]
        
        return random.choice(ctas)

    def generate_hashtags(self, content):
        """T·∫°o hashtags th√¥ng minh d·ª±a tr√™n n·ªôi dung"""
        base_tags = ["#jb88h√†ily", "#JB88h√ÄILY", "#LinkCh√≠nhTh·ª©cjb88h√†ily"]
        
        content_lower = content.lower()
        
        if any(word in content_lower for word in ['b·∫£o m·∫≠t', 'an to√†n', 'ri√™ng t∆∞']):
            base_tags.extend(["#B·∫£oM·∫≠tT·ªëiƒêa", "#AnTo√†nTuy·ªátƒê·ªëi", "#B·∫£oV·ªáTh√¥ngMinh"])
        elif any(word in content_lower for word in ['nhanh', 't·ªëc ƒë·ªô', 'kh·∫©n']):
            base_tags.extend(["#X·ª≠L√ΩNhanh", "#T·ªëcƒê·ªôCao", "#Hi·ªáuSu·∫•tV∆∞·ª£tTr·ªôi"])
        elif any(word in content_lower for word in ['h·ªó tr·ª£', 't∆∞ v·∫•n', 'ƒë·ªìng h√†nh']):
            base_tags.extend(["#H·ªóTr·ª£24/7", "#ChƒÉmS√≥cKh√°chH√†ng", "#T∆∞V·∫•nChuy√™nS√¢u"])
        elif any(word in content_lower for word in ['·ªïn ƒë·ªãnh', 'li√™n t·ª•c', 'th√¥ng su·ªët']):
            base_tags.extend(["#·ªînƒê·ªãnhTuy·ªátƒê·ªëi", "#K·∫øtN·ªëiLi·ªÅnM·∫°ch", "#Hi·ªáuQu·∫£Cao"])
        
        additional_tags = [
            "#UyT√≠n", "#Ch·∫•tL∆∞·ª£ng", "#D·ªãchV·ª•5Sao", "#GameTh·ªß", 
            "#Gi·∫£iTr√≠AnTo√†n", "#C√¥ngNgh·ªáM·ªõi", "#ƒê·∫≥ngC·∫•pQu·ªëcT·∫ø",
            "#LinkChu·∫©n2025", "#H·ªóTr·ª£Nhanh", "#Gi·∫£iPh√°pTo√†nDi·ªán",
            "#Chuy√™nNghi·ªáp", "#TinC·∫≠y", "#MinhB·∫°ch", "#Hi·ªáuQu·∫£"
        ]
        
        base_tags.extend(random.sample(additional_tags, 6))
        return " ".join(base_tags)

    def generate_content(self, keyword, source, user_prompt):
        """T·∫°o n·ªôi dung ho√†n ch·ªânh"""
        # T·∫°o c√°c th√†nh ph·∫ßn th√¥ng minh
        title = self.generate_smart_title()
        description = self.generate_contextual_description()
        benefits = self.generate_dynamic_benefits()
        cta = self.generate_smart_cta(description)
        hashtags = self.generate_hashtags(description)
        
        # X√¢y d·ª±ng n·ªôi dung
        content = f"{title}\n\n"
        content += f"üìû #{keyword} ==> {source}\n\n"
        content += f"{description}\n\n"
        
        # Th√™m ph·∫ßn gi·∫£i th√≠ch v·ªÅ quy tr√¨nh
        process_templates = [
            "Quy tr√¨nh l√†m vi·ªác c·ªßa ch√∫ng t√¥i ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ ƒë·∫£m b·∫£o m·ªçi v·∫•n ƒë·ªÅ ƒë·ªÅu ƒë∆∞·ª£c x·ª≠ l√Ω m·ªôt c√°ch h·ªá th·ªëng v√† hi·ªáu qu·∫£ nh·∫•t.",
            "V·ªõi ph∆∞∆°ng ch√¢m 'kh√°ch h√†ng l√† trung t√¢m', m·ªçi b∆∞·ªõc trong quy tr√¨nh h·ªó tr·ª£ ƒë·ªÅu ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ mang l·∫°i tr·∫£i nghi·ªám t·ªët nh·∫•t.",
            "Ch√∫ng t√¥i lu√¥n c·∫£i ti·∫øn quy tr√¨nh l√†m vi·ªác ƒë·ªÉ ƒë√°p ·ª©ng nhanh ch√≥ng v√† ch√≠nh x√°c m·ªçi y√™u c·∫ßu t·ª´ ph√≠a kh√°ch h√†ng.",
            "M·ªói tr∆∞·ªùng h·ª£p ƒë·ªÅu ƒë∆∞·ª£c ph√¢n lo·∫°i v√† x·ª≠ l√Ω theo quy tr√¨nh chu·∫©n, ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n v√† hi·ªáu qu·∫£ trong gi·∫£i ph√°p."
        ]
        
        content += f"{random.choice(process_templates)}\n\n"
        
        content += "**ƒêi·ªÉm n·ªïi b·∫≠t:**\n"
        for benefit in benefits:
            content += f"- {benefit}\n"
        
        content += f"\n{cta}\n\n"
        
        content += "**Li√™n h·ªá h·ªó tr·ª£:**\n"
        content += "üìû Hotline: 0027395058 (H·ªó tr·ª£ 24/7)\n"
        content += "üì± Telegram: @catten999\n"
        content += "‚è∞ Th·ªùi gian l√†m vi·ªác: 24/7 - K·ªÉ c·∫£ ng√†y l·ªÖ\n\n"
        
        content += f"{hashtags}"
        
        return content

# ------------------------ Anti-dup System ------------------------

def _uniq_load_corpus() -> dict:
    """T·∫£i corpus t·ª´ file"""
    try:
        with open(CORPUS_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def _uniq_save_corpus(corpus: dict):
    """L∆∞u corpus v√†o file"""
    _ensure_dir_for(CORPUS_FILE)
    with open(CORPUS_FILE, "w", encoding="utf-8") as f:
        json.dump(corpus, f, ensure_ascii=False, indent=2)

def _uniq_norm(s: str) -> str:
    """Chu·∫©n h√≥a chu·ªói"""
    s = re.sub(r"\s+", " ", (s or "").strip())
    s = re.sub(r"[‚Äú‚Äù\"'`]+", "", s)
    return s.lower()

def _uniq_tok(s: str):
    """Tokenize chu·ªói"""
    return re.findall(r"[a-zA-Z√Ä-·ªπ0-9]+", s.lower())

def _uniq_ngrams(tokens, n=3):
    """T·∫°o n-grams"""
    return Counter([" ".join(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))])

def _uniq_jaccard(a: str, b: str, n=3) -> float:
    """T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng Jaccard"""
    ta, tb = _uniq_tok(a), _uniq_tok(b)
    sa, sb = set(_uniq_ngrams(ta, n).keys()), set(_uniq_ngrams(tb, n).keys())
    if not sa or not sb: return 0.0
    inter, union = len(sa & sb), len(sa | sb)
    return inter/union if union else 0.0

def _uniq_lev_ratio(a: str, b: str) -> float:
    """T√≠nh t·ª∑ l·ªá Levenshtein"""
    A, B = a, b
    if not A or not B: return 0.0
    la, lb = len(A), len(B)
    dp = list(range(lb+1))
    for i in range(1, la+1):
        prev, dp[0] = dp[0], i
        for j in range(1, lb+1):
            ins = dp[j-1] + 1
            dele = dp[j] + 1
            sub = prev + (0 if A[i-1] == B[j-1] else 1)
            prev, dp[j] = dp[j], min(ins, dele, sub)
    dist = dp[lb]
    maxlen = max(1, la, lb)
    return 1.0 - (dist / maxlen)

def _uniq_too_similar(candidate: str, history: list) -> bool:
    """Ki·ªÉm tra n·ªôi dung tr√πng l·∫∑p"""
    if not history:
        return False
    last = history[0].get("text", "") or ""
    if not last:
        return False
    j = _uniq_jaccard(candidate, last, n=3)
    l = _uniq_lev_ratio(candidate, last)
    return (j >= DUP_J_THRESHOLD or l >= DUP_L_THRESHOLD)

def _uniq_store(page_id: str, text: str):
    """L∆∞u n·ªôi dung v√†o corpus"""
    corpus = _uniq_load_corpus()
    bucket = corpus.get(page_id) or []
    bucket.insert(0, {"text": _uniq_norm(text), "timestamp": time.time()})
    corpus[page_id] = bucket[:100]  # Gi·ªØ 100 b√†i g·∫ßn nh·∫•t
    _uniq_save_corpus(corpus)

# ------------------------ API Routes ------------------------

@app.route("/")
def index():
    """Trang ch·ªß"""
    return make_response(INDEX_HTML)

@app.route("/api/pages")
def api_pages():
    """API l·∫•y danh s√°ch pages"""
    pages = []
    for pid, token in PAGE_TOKENS.items():
        try:
            data = fb_get(pid, {"access_token": token, "fields": "name,id"})
            name = data.get("name", f"Page {pid}")
        except Exception as e:
            name = f"Page {pid} (l·ªói: {str(e)})"
        pages.append({"id": pid, "name": name})
    return jsonify({"data": pages})

# ------------------------ Inbox Management ------------------------

_CONV_CACHE = {}

@app.route("/api/inbox/conversations")
def api_inbox_conversations():
    """API l·∫•y danh s√°ch h·ªôi tho·∫°i"""
    try:
        page_ids = request.args.get("pages", "")
        if not page_ids:
            return jsonify({"data": []})
        page_ids = [p for p in page_ids.split(",") if p]
        only_unread = request.args.get("only_unread") in ("1", "true", "True")
        limit = int(request.args.get("limit", "25"))

        # Cache ƒë·ªÉ t·ªëi ∆∞u hi·ªáu su·∫•t
        key = f"{','.join(sorted(page_ids))}|{int(only_unread)}|{limit}"
        hit = _CONV_CACHE.get(key)
        if hit and hit.get('expire',0) > time.time():
            return jsonify({"data": hit['data']})

        conversations = []
        fields = "updated_time,snippet,senders,unread_count,can_reply,participants,link"
        
        for pid in page_ids:
            token = get_page_token(pid)
            page_name = f"Page {pid}"
            
            try:
                info = fb_get(pid, {"access_token": token, "fields": "name"})
                page_name = info.get("name", page_name)
            except Exception:
                pass

            try:
                data = fb_get(f"{pid}/conversations", {
                    "access_token": token,
                    "limit": limit,
                    "fields": fields,
                })
                
                for c in data.get("data", []):
                    c["page_id"] = pid
                    c["page_name"] = page_name
                    
                    # Extract user_id t·ª´ participants
                    try:
                        parts = c.get("participants", {}).get("data", [])
                        uid = None
                        for p in parts:
                            if p.get("id") != pid:
                                uid = p.get("id")
                                break
                        if uid:
                            c["user_id"] = uid
                    except Exception:
                        pass
                    
                    if only_unread and not c.get("unread_count"):
                        continue
                    conversations.append(c)
                    
            except Exception as e:
                print(f"L·ªói khi l·∫•y h·ªôi tho·∫°i cho page {pid}: {e}")

        conversations.sort(key=lambda c: c.get("updated_time", ""), reverse=True)
        _CONV_CACHE[key] = {"expire": time.time()+12.0, "data": conversations}
        return jsonify({"data": conversations})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/api/inbox/messages")
def api_inbox_messages():
    """API l·∫•y tin nh·∫Øn trong h·ªôi tho·∫°i"""
    try:
        conv_id = request.args.get("conversation_id")
        page_id = request.args.get("page_id")
        
        if not conv_id:
            return jsonify({"data": []})
            
        if page_id:
            token = get_page_token(page_id)
        elif PAGE_TOKENS:
            token = list(PAGE_TOKENS.values())[0]
        else:
            return jsonify({"error": "Kh√¥ng c√≥ PAGE_TOKENS"})
            
        fields = "message,from,to,created_time,id"
        js = fb_get(f"{conv_id}/messages", {
            "access_token": token,
            "limit": 50,
            "fields": fields,
        })
        
        msgs = js.get("data", [])
        page_ids = set(PAGE_TOKENS.keys())
        
        for m in msgs:
            sender_id = None
            if isinstance(m.get("from"), dict):
                sender_id = m["from"].get("id")
            m["is_page"] = sender_id in page_ids
            
        msgs.sort(key=lambda x: x.get("created_time", ""))
        return jsonify({"data": msgs})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/api/inbox/reply", methods=["POST"])
def api_inbox_reply():
    """API g·ª≠i tin nh·∫Øn tr·∫£ l·ªùi"""
    try:
        js = request.get_json(force=True) or {}
        conv_id = js.get("conversation_id")
        page_id = js.get("page_id")
        text = (js.get("text") or "").strip()
        user_id = js.get("user_id")

        if not conv_id and not (page_id and user_id):
            return jsonify({"error": "Thi·∫øu conversation_id ho·∫∑c (page_id + user_id)"})
        if not text:
            return jsonify({"error": "Thi·∫øu n·ªôi dung tin nh·∫Øn"})

        if conv_id:
            token = get_page_token(page_id) if page_id else list(PAGE_TOKENS.values())[0]
            try:
                out = fb_post(f"{conv_id}/messages", {
                    "message": text,
                    "access_token": token,
                })
                return jsonify({"ok": True, "result": out})
            except Exception:
                # Fallback: d√πng Send API
                if page_id and user_id:
                    token = get_page_token(page_id)
                    url = f"{FB_API}/me/messages"
                    r = session.post(url, params={"access_token": token},
                                  json={"recipient": {"id": user_id}, "message": {"text": text}}, 
                                  timeout=30)
                    data = r.json() if r.headers.get("content-type","").startswith("application/json") else {"raw": r.text}
                    if r.status_code >= 400 or "error" in data:
                        raise RuntimeError(f"Send API failed: {data}")
                    return jsonify({"ok": True, "result": data})
                raise

        # Send API direct
        token = get_page_token(page_id)
        url = f"{FB_API}/me/messages"
        r = session.post(url, params={"access_token": token},
                      json={"recipient": {"id": user_id}, "message": {"text": text}}, timeout=30)
        data = r.json() if r.headers.get("content-type","").startswith("application/json") else {"raw": r.text}
        if r.status_code >= 400 or "error" in data:
            raise RuntimeError(f"Send API failed: {data}")
        return jsonify({"ok": True, "result": data})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ------------------------ AI Content Generation ------------------------

_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None

@app.route("/api/ai/generate", methods=["POST"])
def api_ai_generate():
    """API t·∫°o n·ªôi dung b·∫±ng AI"""
    js = request.get_json(force=True) or {}
    page_id = js.get("page_id") or ""
    user_prompt = (js.get("prompt") or "").strip()

    if not page_id:
        return jsonify({"error": "Ch∆∞a ch·ªçn Page"}), 400
    if _client is None:
        return jsonify({"error": "Thi·∫øu OPENAI_API_KEY (ch∆∞a c·∫•u h√¨nh AI)"}), 400

    settings = _load_settings()
    conf = settings.get(page_id) or {}
    keyword = (conf.get("keyword") or "").strip()
    source = (conf.get("source") or "").strip()
    
    if not (keyword or source):
        return jsonify({"error": "Page ch∆∞a c√≥ T·ª´ kho√°/Link ngu·ªìn trong C√†i ƒë·∫∑t"}), 400

    try:
        writer = AIContentWriter(openai_client=_client)
        corpus = _uniq_load_corpus()
        history = corpus.get(page_id) or []
        
        MAX_ATTEMPTS = 3
        last_error = None
        
        for attempt in range(MAX_ATTEMPTS):
            content = writer.generate_content(keyword, source, user_prompt)
            
            # Ki·ªÉm tra ƒë·ªô d√†i
            word_count = len(content.split())
            if word_count < BODY_MIN_WORDS:
                last_error = f"N·ªôi dung qu√° ng·∫Øn ({word_count} t·ª´). C·∫ßn √≠t nh·∫•t {BODY_MIN_WORDS} t·ª´."
                continue
            elif word_count > BODY_MAX_WORDS:
                last_error = f"N·ªôi dung qu√° d√†i ({word_count} t·ª´). T·ªëi ƒëa {BODY_MAX_WORDS} t·ª´."
                continue

            # Anti-dup check
            if ANTI_DUP_ENABLED and _uniq_too_similar(_uniq_norm(content), history):
                last_error = "N·ªôi dung qu√° gi·ªëng v·ªõi b√†i tr∆∞·ªõc"
                continue

            # N·∫øu ƒë·∫°t t·∫•t c·∫£ ƒëi·ªÅu ki·ªán
            _uniq_store(page_id, content)
            return jsonify({
                "text": content,
                "checks": {
                    "similarity": "pass",
                    "word_count": word_count,
                    "attempts": attempt + 1
                }
            })
        
        # N·∫øu v∆∞·ª£t qu√° s·ªë l·∫ßn th·ª≠
        return jsonify({
            "error": f"Kh√¥ng th·ªÉ t·∫°o n·ªôi dung ph√π h·ª£p sau {MAX_ATTEMPTS} l·∫ßn th·ª≠",
            "detail": last_error
        }), 409
        
    except Exception as e:
        return jsonify({"error": f"L·ªói h·ªá th·ªëng: {str(e)}"}), 500

# ------------------------ Media Upload ------------------------

@app.route("/api/upload", methods=["POST"])
def api_upload():
    """API upload media"""
    f = request.files.get("file")
    if not f:
        return jsonify({"error":"Kh√¥ng c√≥ file"})
    
    try:
        # T·∫°o t√™n file duy nh·∫•t
        file_ext = os.path.splitext(f.filename)[1]
        unique_filename = f"{uuid.uuid4().hex}{file_ext}"
        save_path = os.path.join(UPLOAD_FOLDER, unique_filename)
        
        f.save(save_path)
        
        return jsonify({
            "ok": True, 
            "path": save_path,
            "filename": unique_filename,
            "size": os.path.getsize(save_path)
        })
    except Exception as e:
        return jsonify({"error": f"L·ªói upload: {str(e)}"}), 500

# ------------------------ Post to Pages ------------------------

def _build_fallback_link(page_id: str, any_id: str) -> str:
    """T·∫°o fallback link"""
    try:
        if "_" in (any_id or ""):
            pid, postid = any_id.split("_", 1)
            return f"https://www.facebook.com/{pid}/posts/{postid}"
        return f"https://www.facebook.com/{any_id}"
    except Exception:
        return f"https://www.facebook.com/{any_id or page_id}"

def _resolve_permalink(page_id: str, token: str, api_result: dict) -> dict:
    """L·∫•y permalink t·ª´ k·∫øt qu·∫£ API"""
    candidate_ids = []
    for key in ("id", "post_id", "video_id"):
        v = (api_result or {}).get(key)
        if v and v not in candidate_ids:
            candidate_ids.append(v)
            
    post_id = (api_result or {}).get("post_id")
    if post_id and post_id not in candidate_ids:
        candidate_ids.insert(0, post_id)
        
    for cid in candidate_ids:
        try:
            r = fb_get(str(cid), {"access_token": token, "fields": "permalink_url"})
            permalink = r.get("permalink_url")
            if permalink:
                return {"permalink": permalink, "source_id": cid, "fallback": _build_fallback_link(page_id, cid)}
        except Exception:
            continue
            
    fallback_id = candidate_ids[0] if candidate_ids else (api_result.get("id") or page_id)
    return {"permalink": _build_fallback_link(page_id, fallback_id), "source_id": fallback_id, "fallback": _build_fallback_link(page_id, fallback_id)}

@app.route("/api/pages/post", methods=["POST"])
def api_pages_post():
    """API ƒëƒÉng b√†i l√™n pages"""
    try:
        js = request.get_json(force=True) or {}
        pages: t.List[str] = js.get("pages", [])
        text_content = (js.get("text") or "").strip()
        media_url = (js.get("image_url") or js.get("media_url") or "").strip() or None
        media_path = (js.get("media_path") or "").strip() or None
        post_type = (js.get("post_type") or "feed").strip()

        if not pages:
            return jsonify({"error": "Ch·ªçn √≠t nh·∫•t 1 page"})
        if not text_content and not media_url and not media_path:
            return jsonify({"error": "Thi·∫øu n·ªôi dung ho·∫∑c media"})

        results = []
        for pid in pages:
            token = get_page_token(pid)
            is_video = False
            
            # X√°c ƒë·ªãnh lo·∫°i media
            if media_path:
                lower = media_path.lower()
                is_video = lower.endswith(('.mp4','.mov','.mkv','.avi','.webm'))
            elif media_url:
                lower = media_url.lower()
                is_video = any(ext in lower for ext in ['.mp4','.mov','.mkv','.avi','.webm'])

            try:
                if media_path:
                    # Upload t·ª´ local file
                    if is_video:
                        with open(media_path, 'rb') as f:
                            out = session.post(f"{FB_API}/{pid}/videos",
                                params={"access_token": token},
                                files={"source": (os.path.basename(media_path), f)},
                                data={"description": text_content},
                                timeout=(FB_CONNECT_TIMEOUT, FB_READ_TIMEOUT)
                            ).json()
                    else:
                        with open(media_path, 'rb') as f:
                            out = session.post(f"{FB_API}/{pid}/photos",
                                params={"access_token": token},
                                files={"source": (os.path.basename(media_path), f)},
                                data={"caption": text_content},
                                timeout=(FB_CONNECT_TIMEOUT, FB_READ_TIMEOUT)
                            ).json()
                elif media_url:
                    # Upload t·ª´ URL
                    if is_video:
                        out = fb_post(f"{pid}/videos", {
                            "file_url": media_url, 
                            "description": text_content, 
                            "access_token": token
                        })
                    else:
                        out = fb_post(f"{pid}/photos", {
                            "url": media_url, 
                            "caption": text_content, 
                            "access_token": token
                        })
                else:
                    # Ch·ªâ text
                    out = fb_post(f"{pid}/feed", {
                        "message": text_content, 
                        "access_token": token
                    })

                # L·∫•y permalink
                perm = _resolve_permalink(pid, token, out)
                link = perm.get("permalink") or perm.get("fallback")
                
                note = None
                if post_type == 'reels' and not is_video:
                    note = 'Reels y√™u c·∫ßu video; ƒë√£ ƒëƒÉng nh∆∞ Feed do kh√¥ng c√≥ video.'
                    
                results.append({
                    "page_id": pid, 
                    "result": out, 
                    "link": link, 
                    "source_id": perm.get("source_id"), 
                    "note": note
                })
                
            except Exception as e:
                link = None
                try:
                    rid = (locals().get("out") or {}).get("id")
                    if rid: 
                        link = _build_fallback_link(pid, rid)
                except Exception:
                    pass
                results.append({"page_id": pid, "error": str(e), "link": link})
                
        return jsonify({"ok": True, "results": results})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ------------------------ Webhook & SSE ------------------------

@app.route("/webhook/events", methods=["GET","POST"])
def webhook_events():
    """Webhook cho Facebook"""
    if request.method == "GET":
        mode = request.args.get("hub.mode")
        token = request.args.get("hub.verify_token")
        challenge = request.args.get("hub.challenge")
        if mode == "subscribe" and token == VERIFY_TOKEN:
            return Response(challenge, status=200)
        return Response("forbidden", status=403)
    
    # X·ª≠ l√Ω webhook POST
    data = request.get_json()
    print(f"Webhook received: {data}")
    return jsonify({"ok": True})

@app.route("/stream/messages")
def stream_messages():
    """Server-Sent Events cho real-time updates"""
    if DISABLE_SSE:
        return Response("SSE disabled", status=200, mimetype="text/plain")
    
    def gen():
        yield "retry: 15000\n\n"
        while True:
            time.sleep(15)
            yield "data: {}\n\n"
            
    return Response(gen(), mimetype="text/event-stream")

# ------------------------ Settings Management ------------------------

@app.route("/api/settings/get")
def api_settings_get():
    """API l·∫•y c√†i ƒë·∫∑t"""
    try:
        data = _load_settings()
        rows = []
        for pid, token in PAGE_TOKENS.items():
            try:
                info = fb_get(pid, {"access_token": token, "fields": "name"})
                name = info.get("name", f"Page {pid}")
            except Exception:
                name = f"Page {pid}"
            conf = data.get(pid) or {}
            rows.append({
                "id": pid, 
                "name": name, 
                "keyword": conf.get("keyword", ""), 
                "source": conf.get("source", "")
            })
        return jsonify({"data": rows})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/api/settings/save", methods=["POST"])
def api_settings_save():
    """API l∆∞u c√†i ƒë·∫∑t"""
    try:
        js = request.get_json(force=True) or {}
        items = js.get("items") or []
        if not isinstance(items, list):
            return jsonify({"error": "payload kh√¥ng h·ª£p l·ªá"}), 400
            
        data = _load_settings()
        updated = 0
        
        for it in items:
            pid = (it.get("id") or "").strip()
            if not pid or pid not in PAGE_TOKENS:
                continue
                
            kw = (it.get("keyword") or "").strip()
            src = (it.get("source") or "").strip()
            
            if pid not in data:
                data[pid] = {}
                
            data[pid]["keyword"] = kw
            data[pid]["source"]  = src
            updated += 1
            
        _save_settings(data)
        return jsonify({"ok": True, "updated": updated})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# ------------------------ CSV Export/Import ------------------------

@app.route("/api/settings/export")
def api_settings_export_v2():
    """API export c√†i ƒë·∫∑t sang CSV"""
    from io import StringIO
    output = StringIO()
    writer = csv.writer(output)
    writer.writerow(["id","name","keyword","source"])
    
    data = _load_settings()
    for pid, token in PAGE_TOKENS.items():
        try:
            info = fb_get(pid, {"access_token": token, "fields": "name"})
            name = info.get("name", f"Page {pid}")
        except Exception:
            name = f"Page {pid}"
            
        conf = data.get(pid) or {}
        writer.writerow([pid, name, conf.get("keyword",""), conf.get("source","")])
        
    csv_text = output.getvalue()
    return Response(
        csv_text, 
        mimetype="text/csv",
        headers={"Content-Disposition": "attachment; filename=settings.csv"}
    )

@app.route("/api/settings/import", methods=["POST"])
def api_settings_import_v2():
    """API import c√†i ƒë·∫∑t t·ª´ CSV"""
    file = request.files.get("file")
    if not file:
        return jsonify({"error": "Thi·∫øu file CSV"})
        
    try:
        content = file.read().decode("utf-8", errors="ignore")
        rdr = csv.DictReader(content.splitlines())
        data = _load_settings()
        count = 0
        
        for row in rdr:
            pid = (row.get("id") or "").strip()
            if not pid:
                continue
            if pid not in PAGE_TOKENS:
                continue
                
            keyword = (row.get("keyword") or row.get("tukhoa") or "").strip()
            source  = (row.get("source")  or row.get("link")   or "").strip()
            
            if pid not in data:
                data[pid] = {}
                
            if keyword or source:
                data[pid]["keyword"] = keyword
                data[pid]["source"]  = source
                count += 1
                
        _save_settings(data)
        return jsonify({"ok": True, "updated": count})
    except Exception as e:
        return jsonify({"error": f"L·ªói import: {str(e)}"}), 500

# ------------------------ Admin Tools ------------------------

@app.route("/admin/corpus-info")
def admin_corpus_info():
    """API th√¥ng tin corpus (admin only)"""
    key = request.args.get("key", "")
    if key != SECRET_KEY:
        return jsonify({"error": "forbidden"}), 403
        
    try:
        data = _uniq_load_corpus()
        info = {pid: len(items or []) for pid, items in data.items()}
        return jsonify({"ok": True, "pages": info, "path": CORPUS_FILE})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/admin/reset-corpus", methods=["POST", "GET"])
def admin_reset_corpus():
    """API reset corpus (admin only)"""
    key = request.args.get("key", "")
    if key != SECRET_KEY:
        return jsonify({"error": "forbidden"}), 403
        
    try:
        size = 0
        if os.path.exists(CORPUS_FILE):
            size = os.path.getsize(CORPUS_FILE)
            os.remove(CORPUS_FILE)
        _uniq_save_corpus({})
        return jsonify({"ok": True, "deleted_bytes": size, "path": CORPUS_FILE})
    except Exception as e:
        return jsonify({"error": str(e)}), 500

@app.route("/health")
def health_check():
    """Health check endpoint"""
    return jsonify({
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "pages_count": len(PAGE_TOKENS),
        "openai_configured": _client is not None
    })

# ------------------------ Error Handlers ------------------------

@app.errorhandler(404)
def not_found(error):
    return jsonify({"error": "Endpoint kh√¥ng t·ªìn t·∫°i"}), 404

@app.errorhandler(500)
def internal_error(error):
    return jsonify({"error": "L·ªói m√°y ch·ªß n·ªôi b·ªô"}), 500

# ------------------------ Main Entry Point ------------------------

if __name__ == "__main__":
    port = int(os.getenv("PORT", "5000"))
    debug_mode = os.getenv("DEBUG", "false").lower() == "true"
    
    print(f"üöÄ Kh·ªüi ch·∫°y AKUTA Content Manager 2025")
    print(f"üìç Port: {port}")
    print(f"üîß Debug: {debug_mode}")
    print(f"üìä S·ªë pages: {len(PAGE_TOKENS)}")
    print(f"ü§ñ OpenAI: {'‚úÖ ƒê√£ c·∫•u h√¨nh' if _client else '‚ùå Ch∆∞a c·∫•u h√¨nh'}")
    
    app.run(host="0.0.0.0", port=port, debug=debug_mode)
